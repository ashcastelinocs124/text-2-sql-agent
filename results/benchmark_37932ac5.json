{
  "benchmark_id": "37932ac5",
  "started_at": "2026-01-02T20:17:02.085143+00:00",
  "completed_at": "2026-01-02T20:17:02.345053+00:00",
  "duration_seconds": 0.25991,
  "config": {
    "agent_url": null,
    "tasks_path": null,
    "output_dir": "results/",
    "difficulties": [
      "easy",
      "medium",
      "hard",
      "enterprise"
    ],
    "tags": null,
    "formats": [
      "json",
      "csv",
      "summary",
      "html"
    ],
    "dialect": "sqlite",
    "timeout": 30.0,
    "verbose": false,
    "schema": "enterprise"
  },
  "total_tasks": 31,
  "successful": 28,
  "failed": 3,
  "errors": 0,
  "skipped": 0,
  "average_score": 0.9983035714285714,
  "median_score": 0.9974999999999999,
  "min_score": 0.995,
  "max_score": 1.0,
  "scores_by_dimension": {
    "correctness": 1.0,
    "efficiency": 1.0,
    "safety": 1.0,
    "completeness": 1.0,
    "semantic_accuracy": 1.0,
    "best_practices": 0.9660714285714286,
    "plan_quality": 1.0
  },
  "scores_by_difficulty": {
    "enterprise": {
      "count": 28,
      "average": 0.9983035714285714,
      "min": 0.995,
      "max": 1.0
    }
  },
  "scores_by_tag": {
    "time_series": {
      "count": 3,
      "average": 0.9974999999999999
    },
    "ltv": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "lead": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "window": {
      "count": 9,
      "average": 0.9977777777777779
    },
    "gaps_islands": {
      "count": 2,
      "average": 0.99875
    },
    "streak": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "fact_table": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "ntile": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "partition": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "sessionization": {
      "count": 1,
      "average": 0.995
    },
    "multi_tenant": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "isolation": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "monitoring": {
      "count": 2,
      "average": 1.0
    },
    "user_behavior": {
      "count": 1,
      "average": 0.995
    },
    "churn": {
      "count": 1,
      "average": 1.0
    },
    "pareto": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "conversion": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "crosstab": {
      "count": 1,
      "average": 1.0
    },
    "reporting": {
      "count": 2,
      "average": 0.99875
    },
    "data_quality": {
      "count": 3,
      "average": 1.0
    },
    "lag": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "tree": {
      "count": 1,
      "average": 1.0
    },
    "customer_analytics": {
      "count": 2,
      "average": 0.99875
    },
    "inventory": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "point_in_time": {
      "count": 1,
      "average": 1.0
    },
    "percentile": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "segmentation": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "merge": {
      "count": 1,
      "average": 0.995
    },
    "attribution": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "cohort": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "analytics": {
      "count": 7,
      "average": 0.9974999999999999
    },
    "abc_analysis": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "validation": {
      "count": 1,
      "average": 1.0
    },
    "month_over_month": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "marketing": {
      "count": 2,
      "average": 0.9974999999999999
    },
    "retention": {
      "count": 2,
      "average": 0.99875
    },
    "consecutive": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "recursive": {
      "count": 2,
      "average": 1.0
    },
    "dimension": {
      "count": 3,
      "average": 0.9974999999999999
    },
    "classification": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "star_schema": {
      "count": 2,
      "average": 0.9974999999999999
    },
    "cte": {
      "count": 3,
      "average": 0.9991666666666666
    },
    "financial": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "current_record": {
      "count": 1,
      "average": 1.0
    },
    "sequence": {
      "count": 1,
      "average": 1.0
    },
    "scd": {
      "count": 4,
      "average": 0.99875
    },
    "growth": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "history": {
      "count": 2,
      "average": 1.0
    },
    "yoy": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "retail": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "manufacturing": {
      "count": 1,
      "average": 1.0
    },
    "association": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "pivot": {
      "count": 1,
      "average": 1.0
    },
    "bom": {
      "count": 1,
      "average": 1.0
    },
    "prediction": {
      "count": 1,
      "average": 1.0
    },
    "scd_type2": {
      "count": 2,
      "average": 1.0
    },
    "variance": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "statistics": {
      "count": 1,
      "average": 1.0
    },
    "customer_360": {
      "count": 1,
      "average": 1.0
    },
    "temporal": {
      "count": 1,
      "average": 1.0
    },
    "hierarchy": {
      "count": 1,
      "average": 1.0
    },
    "anomaly": {
      "count": 1,
      "average": 1.0
    },
    "funnel": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "running_total": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "integration": {
      "count": 1,
      "average": 1.0
    },
    "moving_average": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "crm": {
      "count": 1,
      "average": 1.0
    },
    "percent_rank": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "aggregation": {
      "count": 4,
      "average": 0.9981249999999999
    },
    "etl": {
      "count": 2,
      "average": 0.9975
    },
    "market_basket": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "saas": {
      "count": 1,
      "average": 0.9974999999999999
    },
    "multi_touch": {
      "count": 1,
      "average": 0.9974999999999999
    }
  },
  "results": [
    {
      "task_id": "ent_star_schema_basic",
      "question": "Query the star schema to get total sales by product category and store region",
      "difficulty": "enterprise",
      "tags": [
        "star_schema",
        "fact_table",
        "dimension",
        "aggregation"
      ],
      "gold_sql": "SELECT dp.category, ds.region, SUM(sf.quantity * sf.unit_price) as total_sales FROM sales_fact sf JOIN dim_product dp ON sf.product_id = dp.product_id JOIN dim_store ds ON sf.store_id = ds.store_id GROUP BY dp.category, ds.region ORDER BY total_sales DESC",
      "agent_sql": "SELECT dp.category, ds.region, SUM(sf.quantity * sf.unit_price) as total_sales FROM sales_fact sf JOIN dim_product dp ON sf.product_id = dp.product_id JOIN dim_store ds ON sf.store_id = ds.store_id GROUP BY dp.category, ds.region ORDER BY total_sales DESC",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.32329559326171875,
      "rows_returned": 25,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause",
        "Use table aliases for joined tables"
      ]
    },
    {
      "task_id": "ent_star_schema_time",
      "question": "Analyze quarterly sales trends by product category with year-over-year comparison",
      "difficulty": "enterprise",
      "tags": [
        "star_schema",
        "yoy",
        "time_series",
        "cte"
      ],
      "gold_sql": "WITH quarterly_sales AS (SELECT dd.year, dd.quarter, dp.category, SUM(sf.quantity * sf.unit_price) as sales FROM sales_fact sf JOIN dim_date dd ON sf.date_id = dd.date_id JOIN dim_product dp ON sf.product_id = dp.product_id GROUP BY dd.year, dd.quarter, dp.category) SELECT curr.year, curr.quarter, curr.category, curr.sales as current_sales, prev.sales as prev_year_sales, ROUND((curr.sales - COALESCE(prev.sales, 0)) * 100.0 / NULLIF(prev.sales, 0), 2) as yoy_growth FROM quarterly_sales curr LEFT JOIN quarterly_sales prev ON curr.category = prev.category AND curr.quarter = prev.quarter AND curr.year = prev.year + 1 ORDER BY curr.year, curr.quarter, curr.category",
      "agent_sql": "WITH quarterly_sales AS (SELECT dd.year, dd.quarter, dp.category, SUM(sf.quantity * sf.unit_price) as sales FROM sales_fact sf JOIN dim_date dd ON sf.date_id = dd.date_id JOIN dim_product dp ON sf.product_id = dp.product_id GROUP BY dd.year, dd.quarter, dp.category) SELECT curr.year, curr.quarter, curr.category, curr.sales as current_sales, prev.sales as prev_year_sales, ROUND((curr.sales - COALESCE(prev.sales, 0)) * 100.0 / NULLIF(prev.sales, 0), 2) as yoy_growth FROM quarterly_sales curr LEFT JOIN quarterly_sales prev ON curr.category = prev.category AND curr.quarter = prev.quarter AND curr.year = prev.year + 1 ORDER BY curr.year, curr.quarter, curr.category",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.3521442413330078,
      "rows_returned": 20,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause"
      ]
    },
    {
      "task_id": "ent_scd_type2_current",
      "question": "Get the current version of all customer records from a Type 2 slowly changing dimension",
      "difficulty": "enterprise",
      "tags": [
        "scd",
        "scd_type2",
        "dimension",
        "current_record"
      ],
      "gold_sql": "SELECT customer_id, customer_name, email, segment, valid_from, valid_to FROM dim_customer_scd WHERE is_current = 1 ORDER BY customer_id",
      "agent_sql": "SELECT customer_id, customer_name, email, segment, valid_from, valid_to FROM dim_customer_scd WHERE is_current = 1 ORDER BY customer_id",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.027179718017578125,
      "rows_returned": 3,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_scd_type2_history",
      "question": "Track the complete history of changes for a specific customer in SCD Type 2",
      "difficulty": "enterprise",
      "tags": [
        "scd",
        "scd_type2",
        "history",
        "window"
      ],
      "gold_sql": "SELECT surrogate_key, customer_id, customer_name, email, segment, valid_from, valid_to, is_current, ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY valid_from) as version_number FROM dim_customer_scd WHERE customer_id = 1001 ORDER BY valid_from",
      "agent_sql": "SELECT surrogate_key, customer_id, customer_name, email, segment, valid_from, valid_to, is_current, ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY valid_from) as version_number FROM dim_customer_scd WHERE customer_id = 1001 ORDER BY valid_from",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.04601478576660156,
      "rows_returned": 3,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_scd_point_in_time",
      "question": "Get customer data as it existed on a specific historical date (point-in-time query)",
      "difficulty": "enterprise",
      "tags": [
        "scd",
        "point_in_time",
        "temporal",
        "history"
      ],
      "gold_sql": "SELECT customer_id, customer_name, segment, valid_from, valid_to FROM dim_customer_scd WHERE valid_from <= '2024-06-15' AND (valid_to > '2024-06-15' OR valid_to IS NULL) ORDER BY customer_id",
      "agent_sql": "SELECT customer_id, customer_name, segment, valid_from, valid_to FROM dim_customer_scd WHERE valid_from <= '2024-06-15' AND (valid_to > '2024-06-15' OR valid_to IS NULL) ORDER BY customer_id",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.018835067749023438,
      "rows_returned": 3,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_window_running_total_partition",
      "question": "Calculate running total of sales partitioned by store with daily granularity",
      "difficulty": "enterprise",
      "tags": [
        "window",
        "running_total",
        "partition",
        "aggregation"
      ],
      "gold_sql": "SELECT sf.store_id, ds.store_name, dd.full_date, SUM(sf.quantity * sf.unit_price) as daily_sales, SUM(SUM(sf.quantity * sf.unit_price)) OVER (PARTITION BY sf.store_id ORDER BY dd.full_date ROWS UNBOUNDED PRECEDING) as running_total FROM sales_fact sf JOIN dim_store ds ON sf.store_id = ds.store_id JOIN dim_date dd ON sf.date_id = dd.date_id GROUP BY sf.store_id, ds.store_name, dd.full_date ORDER BY sf.store_id, dd.full_date",
      "agent_sql": "SELECT sf.store_id, ds.store_name, dd.full_date, SUM(sf.quantity * sf.unit_price) as daily_sales, SUM(SUM(sf.quantity * sf.unit_price)) OVER (PARTITION BY sf.store_id ORDER BY dd.full_date ROWS UNBOUNDED PRECEDING) as running_total FROM sales_fact sf JOIN dim_store ds ON sf.store_id = ds.store_id JOIN dim_date dd ON sf.date_id = dd.date_id GROUP BY sf.store_id, ds.store_name, dd.full_date ORDER BY sf.store_id, dd.full_date",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.8730888366699219,
      "rows_returned": 437,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause"
      ]
    },
    {
      "task_id": "ent_window_moving_average",
      "question": "Calculate 7-day moving average of sales for trend analysis",
      "difficulty": "enterprise",
      "tags": [
        "window",
        "moving_average",
        "time_series",
        "analytics"
      ],
      "gold_sql": "WITH daily_sales AS (SELECT dd.full_date, SUM(sf.quantity * sf.unit_price) as sales FROM sales_fact sf JOIN dim_date dd ON sf.date_id = dd.date_id GROUP BY dd.full_date) SELECT full_date, sales, ROUND(AVG(sales) OVER (ORDER BY full_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW), 2) as moving_avg_7d, ROUND(AVG(sales) OVER (ORDER BY full_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW), 2) as moving_avg_30d FROM daily_sales ORDER BY full_date",
      "agent_sql": "WITH daily_sales AS (SELECT dd.full_date, SUM(sf.quantity * sf.unit_price) as sales FROM sales_fact sf JOIN dim_date dd ON sf.date_id = dd.date_id GROUP BY dd.full_date) SELECT full_date, sales, ROUND(AVG(sales) OVER (ORDER BY full_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW), 2) as moving_avg_7d, ROUND(AVG(sales) OVER (ORDER BY full_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW), 2) as moving_avg_30d FROM daily_sales ORDER BY full_date",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.7350444793701172,
      "rows_returned": 245,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause"
      ]
    },
    {
      "task_id": "ent_window_lag_lead",
      "question": "Compare each day's sales with previous and next day using LAG and LEAD",
      "difficulty": "enterprise",
      "tags": [
        "window",
        "lag",
        "lead",
        "time_series"
      ],
      "gold_sql": "WITH daily_sales AS (SELECT dd.full_date, SUM(sf.quantity * sf.unit_price) as sales FROM sales_fact sf JOIN dim_date dd ON sf.date_id = dd.date_id GROUP BY dd.full_date) SELECT full_date, sales, LAG(sales, 1) OVER (ORDER BY full_date) as prev_day_sales, LEAD(sales, 1) OVER (ORDER BY full_date) as next_day_sales, ROUND((sales - LAG(sales, 1) OVER (ORDER BY full_date)) * 100.0 / NULLIF(LAG(sales, 1) OVER (ORDER BY full_date), 0), 2) as day_over_day_pct FROM daily_sales ORDER BY full_date",
      "agent_sql": "WITH daily_sales AS (SELECT dd.full_date, SUM(sf.quantity * sf.unit_price) as sales FROM sales_fact sf JOIN dim_date dd ON sf.date_id = dd.date_id GROUP BY dd.full_date) SELECT full_date, sales, LAG(sales, 1) OVER (ORDER BY full_date) as prev_day_sales, LEAD(sales, 1) OVER (ORDER BY full_date) as next_day_sales, ROUND((sales - LAG(sales, 1) OVER (ORDER BY full_date)) * 100.0 / NULLIF(LAG(sales, 1) OVER (ORDER BY full_date), 0), 2) as day_over_day_pct FROM daily_sales ORDER BY full_date",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.7522106170654297,
      "rows_returned": 245,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause"
      ]
    },
    {
      "task_id": "ent_window_ntile",
      "question": "Segment customers into quartiles based on lifetime value using NTILE",
      "difficulty": "enterprise",
      "tags": [
        "window",
        "ntile",
        "segmentation",
        "ltv"
      ],
      "gold_sql": "WITH customer_ltv AS (SELECT sf.customer_id, dc.customer_name, SUM(sf.quantity * sf.unit_price) as lifetime_value FROM sales_fact sf JOIN dim_customer dc ON sf.customer_id = dc.customer_id GROUP BY sf.customer_id, dc.customer_name) SELECT customer_id, customer_name, lifetime_value, NTILE(4) OVER (ORDER BY lifetime_value DESC) as ltv_quartile, CASE NTILE(4) OVER (ORDER BY lifetime_value DESC) WHEN 1 THEN 'Platinum' WHEN 2 THEN 'Gold' WHEN 3 THEN 'Silver' ELSE 'Bronze' END as tier FROM customer_ltv ORDER BY lifetime_value DESC",
      "agent_sql": "WITH customer_ltv AS (SELECT sf.customer_id, dc.customer_name, SUM(sf.quantity * sf.unit_price) as lifetime_value FROM sales_fact sf JOIN dim_customer dc ON sf.customer_id = dc.customer_id GROUP BY sf.customer_id, dc.customer_name) SELECT customer_id, customer_name, lifetime_value, NTILE(4) OVER (ORDER BY lifetime_value DESC) as ltv_quartile, CASE NTILE(4) OVER (ORDER BY lifetime_value DESC) WHEN 1 THEN 'Platinum' WHEN 2 THEN 'Gold' WHEN 3 THEN 'Silver' ELSE 'Bronze' END as tier FROM customer_ltv ORDER BY lifetime_value DESC",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.20313262939453125,
      "rows_returned": 50,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause"
      ]
    },
    {
      "task_id": "ent_window_percent_rank",
      "question": "Calculate percentile ranking of products by revenue",
      "difficulty": "enterprise",
      "tags": [
        "window",
        "percent_rank",
        "percentile",
        "analytics"
      ],
      "gold_sql": "WITH product_revenue AS (SELECT dp.product_id, dp.product_name, dp.category, SUM(sf.quantity * sf.unit_price) as revenue FROM sales_fact sf JOIN dim_product dp ON sf.product_id = dp.product_id GROUP BY dp.product_id, dp.product_name, dp.category) SELECT product_id, product_name, category, revenue, ROUND(PERCENT_RANK() OVER (ORDER BY revenue) * 100, 2) as percentile, ROUND(PERCENT_RANK() OVER (PARTITION BY category ORDER BY revenue) * 100, 2) as category_percentile FROM product_revenue ORDER BY revenue DESC",
      "agent_sql": "WITH product_revenue AS (SELECT dp.product_id, dp.product_name, dp.category, SUM(sf.quantity * sf.unit_price) as revenue FROM sales_fact sf JOIN dim_product dp ON sf.product_id = dp.product_id GROUP BY dp.product_id, dp.product_name, dp.category) SELECT product_id, product_name, category, revenue, ROUND(PERCENT_RANK() OVER (ORDER BY revenue) * 100, 2) as percentile, ROUND(PERCENT_RANK() OVER (PARTITION BY category ORDER BY revenue) * 100, 2) as category_percentile FROM product_revenue ORDER BY revenue DESC",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.3571510314941406,
      "rows_returned": 30,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause"
      ]
    },
    {
      "task_id": "ent_funnel_analysis",
      "question": "Analyze conversion funnel: page views -> add to cart -> checkout -> purchase",
      "difficulty": "enterprise",
      "tags": [
        "funnel",
        "conversion",
        "analytics",
        "marketing"
      ],
      "gold_sql": "WITH funnel_stages AS (SELECT user_id, MAX(CASE WHEN event_type = 'page_view' THEN 1 ELSE 0 END) as viewed, MAX(CASE WHEN event_type = 'add_to_cart' THEN 1 ELSE 0 END) as added_to_cart, MAX(CASE WHEN event_type = 'checkout' THEN 1 ELSE 0 END) as checked_out, MAX(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchased FROM user_events GROUP BY user_id) SELECT SUM(viewed) as page_views, SUM(added_to_cart) as cart_adds, SUM(checked_out) as checkouts, SUM(purchased) as purchases, ROUND(SUM(added_to_cart) * 100.0 / NULLIF(SUM(viewed), 0), 2) as view_to_cart_pct, ROUND(SUM(checked_out) * 100.0 / NULLIF(SUM(added_to_cart), 0), 2) as cart_to_checkout_pct, ROUND(SUM(purchased) * 100.0 / NULLIF(SUM(checked_out), 0), 2) as checkout_to_purchase_pct, ROUND(SUM(purchased) * 100.0 / NULLIF(SUM(viewed), 0), 2) as overall_conversion_pct FROM funnel_stages",
      "agent_sql": "WITH funnel_stages AS (SELECT user_id, MAX(CASE WHEN event_type = 'page_view' THEN 1 ELSE 0 END) as viewed, MAX(CASE WHEN event_type = 'add_to_cart' THEN 1 ELSE 0 END) as added_to_cart, MAX(CASE WHEN event_type = 'checkout' THEN 1 ELSE 0 END) as checked_out, MAX(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchased FROM user_events GROUP BY user_id) SELECT SUM(viewed) as page_views, SUM(added_to_cart) as cart_adds, SUM(checked_out) as checkouts, SUM(purchased) as purchases, ROUND(SUM(added_to_cart) * 100.0 / NULLIF(SUM(viewed), 0), 2) as view_to_cart_pct, ROUND(SUM(checked_out) * 100.0 / NULLIF(SUM(added_to_cart), 0), 2) as cart_to_checkout_pct, ROUND(SUM(purchased) * 100.0 / NULLIF(SUM(checked_out), 0), 2) as checkout_to_purchase_pct, ROUND(SUM(purchased) * 100.0 / NULLIF(SUM(viewed), 0), 2) as overall_conversion_pct FROM funnel_stages",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.28967857360839844,
      "rows_returned": 1,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause"
      ]
    },
    {
      "task_id": "ent_sessionization",
      "question": "Sessionize user events with 30-minute inactivity timeout",
      "difficulty": "enterprise",
      "tags": [
        "sessionization",
        "window",
        "analytics",
        "user_behavior"
      ],
      "gold_sql": "WITH time_diffs AS (SELECT user_id, event_id, event_type, event_timestamp, LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp) as prev_timestamp, CASE WHEN (julianday(event_timestamp) - julianday(LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp))) * 24 * 60 > 30 OR LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp) IS NULL THEN 1 ELSE 0 END as new_session FROM user_events), sessions AS (SELECT *, SUM(new_session) OVER (PARTITION BY user_id ORDER BY event_timestamp) as session_id FROM time_diffs) SELECT user_id, session_id, MIN(event_timestamp) as session_start, MAX(event_timestamp) as session_end, COUNT(*) as events_in_session, ROUND((julianday(MAX(event_timestamp)) - julianday(MIN(event_timestamp))) * 24 * 60, 2) as session_duration_mins FROM sessions GROUP BY user_id, session_id ORDER BY user_id, session_id",
      "agent_sql": "WITH time_diffs AS (SELECT user_id, event_id, event_type, event_timestamp, LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp) as prev_timestamp, CASE WHEN (julianday(event_timestamp) - julianday(LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp))) * 24 * 60 > 30 OR LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp) IS NULL THEN 1 ELSE 0 END as new_session FROM user_events), sessions AS (SELECT *, SUM(new_session) OVER (PARTITION BY user_id ORDER BY event_timestamp) as session_id FROM time_diffs) SELECT user_id, session_id, MIN(event_timestamp) as session_start, MAX(event_timestamp) as session_end, COUNT(*) as events_in_session, ROUND((julianday(MAX(event_timestamp)) - julianday(MIN(event_timestamp))) * 24 * 60, 2) as session_duration_mins FROM sessions GROUP BY user_id, session_id ORDER BY user_id, session_id",
      "status": "success",
      "overall_score": 0.995,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.9,
      "plan_quality": 1.0,
      "execution_time_ms": 3.8831233978271484,
      "rows_returned": 994,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Specify only the columns you need"
      ]
    },
    {
      "task_id": "ent_gap_islands",
      "question": "Find gaps in sequential order numbers (missing orders)",
      "difficulty": "enterprise",
      "tags": [
        "gaps_islands",
        "sequence",
        "data_quality",
        "window"
      ],
      "gold_sql": "WITH order_sequence AS (SELECT order_id, LEAD(order_id) OVER (ORDER BY order_id) as next_order_id FROM orders_fact), gaps AS (SELECT order_id as gap_start, next_order_id as gap_end, next_order_id - order_id - 1 as gap_size FROM order_sequence WHERE next_order_id - order_id > 1) SELECT gap_start + 1 as missing_from, gap_end - 1 as missing_to, gap_size as missing_count FROM gaps ORDER BY gap_start",
      "agent_sql": "WITH order_sequence AS (SELECT order_id, LEAD(order_id) OVER (ORDER BY order_id) as next_order_id FROM orders_fact), gaps AS (SELECT order_id as gap_start, next_order_id as gap_end, next_order_id - order_id - 1 as gap_size FROM order_sequence WHERE next_order_id - order_id > 1) SELECT gap_start + 1 as missing_from, gap_end - 1 as missing_to, gap_size as missing_count FROM gaps ORDER BY gap_start",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.16999244689941406,
      "rows_returned": 0,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_consecutive_days",
      "question": "Find customers with consecutive days of purchases (streak analysis)",
      "difficulty": "enterprise",
      "tags": [
        "gaps_islands",
        "streak",
        "consecutive",
        "window"
      ],
      "gold_sql": "WITH daily_purchases AS (SELECT DISTINCT customer_id, DATE(order_date) as purchase_date FROM orders_fact), date_groups AS (SELECT customer_id, purchase_date, DATE(purchase_date, '-' || ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY purchase_date) || ' days') as grp FROM daily_purchases), streaks AS (SELECT customer_id, MIN(purchase_date) as streak_start, MAX(purchase_date) as streak_end, COUNT(*) as streak_length FROM date_groups GROUP BY customer_id, grp) SELECT dc.customer_name, s.streak_start, s.streak_end, s.streak_length FROM streaks s JOIN dim_customer dc ON s.customer_id = dc.customer_id WHERE s.streak_length >= 3 ORDER BY s.streak_length DESC, s.streak_start",
      "agent_sql": "WITH daily_purchases AS (SELECT DISTINCT customer_id, DATE(order_date) as purchase_date FROM orders_fact), date_groups AS (SELECT customer_id, purchase_date, DATE(purchase_date, '-' || ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY purchase_date) || ' days') as grp FROM daily_purchases), streaks AS (SELECT customer_id, MIN(purchase_date) as streak_start, MAX(purchase_date) as streak_end, COUNT(*) as streak_length FROM date_groups GROUP BY customer_id, grp) SELECT dc.customer_name, s.streak_start, s.streak_end, s.streak_length FROM streaks s JOIN dim_customer dc ON s.customer_id = dc.customer_id WHERE s.streak_length >= 3 ORDER BY s.streak_length DESC, s.streak_start",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.5497932434082031,
      "rows_returned": 0,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_multi_tenant",
      "question": "Query sales data with multi-tenant isolation (tenant-specific aggregations)",
      "difficulty": "enterprise",
      "tags": [
        "multi_tenant",
        "isolation",
        "saas",
        "aggregation"
      ],
      "gold_sql": "WITH tenant_sales AS (SELECT t.tenant_id, t.tenant_name, dp.category, SUM(sf.quantity * sf.unit_price) as total_sales, COUNT(DISTINCT sf.customer_id) as unique_customers, COUNT(DISTINCT sf.transaction_id) as total_transactions FROM sales_fact sf JOIN tenants t ON sf.tenant_id = t.tenant_id JOIN dim_product dp ON sf.product_id = dp.product_id GROUP BY t.tenant_id, t.tenant_name, dp.category) SELECT tenant_name, category, total_sales, unique_customers, total_transactions, ROUND(total_sales * 100.0 / SUM(total_sales) OVER (PARTITION BY tenant_id), 2) as pct_of_tenant_sales, RANK() OVER (PARTITION BY tenant_id ORDER BY total_sales DESC) as category_rank FROM tenant_sales ORDER BY tenant_id, total_sales DESC",
      "agent_sql": "WITH tenant_sales AS (SELECT t.tenant_id, t.tenant_name, dp.category, SUM(sf.quantity * sf.unit_price) as total_sales, COUNT(DISTINCT sf.customer_id) as unique_customers, COUNT(DISTINCT sf.transaction_id) as total_transactions FROM sales_fact sf JOIN tenants t ON sf.tenant_id = t.tenant_id JOIN dim_product dp ON sf.product_id = dp.product_id GROUP BY t.tenant_id, t.tenant_name, dp.category) SELECT tenant_name, category, total_sales, unique_customers, total_transactions, ROUND(total_sales * 100.0 / SUM(total_sales) OVER (PARTITION BY tenant_id), 2) as pct_of_tenant_sales, RANK() OVER (PARTITION BY tenant_id ORDER BY total_sales DESC) as category_rank FROM tenant_sales ORDER BY tenant_id, total_sales DESC",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.46372413635253906,
      "rows_returned": 15,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_recursive_hierarchy",
      "question": "Traverse organizational hierarchy using recursive CTE",
      "difficulty": "enterprise",
      "tags": [
        "recursive",
        "cte",
        "hierarchy",
        "tree"
      ],
      "gold_sql": "WITH RECURSIVE org_tree AS (SELECT employee_id, employee_name, manager_id, title, 1 as level, employee_name as path FROM employees WHERE manager_id IS NULL UNION ALL SELECT e.employee_id, e.employee_name, e.manager_id, e.title, ot.level + 1, ot.path || ' > ' || e.employee_name FROM employees e JOIN org_tree ot ON e.manager_id = ot.employee_id) SELECT employee_id, employee_name, title, level, path, (SELECT COUNT(*) FROM employees WHERE manager_id = org_tree.employee_id) as direct_reports FROM org_tree ORDER BY path",
      "agent_sql": "WITH RECURSIVE org_tree AS (SELECT employee_id, employee_name, manager_id, title, 1 as level, employee_name as path FROM employees WHERE manager_id IS NULL UNION ALL SELECT e.employee_id, e.employee_name, e.manager_id, e.title, ot.level + 1, ot.path || ' > ' || e.employee_name FROM employees e JOIN org_tree ot ON e.manager_id = ot.employee_id) SELECT employee_id, employee_name, title, level, path, (SELECT COUNT(*) FROM employees WHERE manager_id = org_tree.employee_id) as direct_reports FROM org_tree ORDER BY path",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.08392333984375,
      "rows_returned": 10,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_recursive_bom",
      "question": "Calculate total cost of a product using Bill of Materials (BOM) explosion",
      "difficulty": "enterprise",
      "tags": [
        "recursive",
        "cte",
        "bom",
        "manufacturing"
      ],
      "gold_sql": "WITH RECURSIVE bom_explosion AS (SELECT parent_product_id, component_id, quantity, 1 as level FROM bill_of_materials WHERE parent_product_id = 'PROD001' UNION ALL SELECT bom.parent_product_id, bom.component_id, bom.quantity * be.quantity, be.level + 1 FROM bill_of_materials bom JOIN bom_explosion be ON bom.parent_product_id = be.component_id WHERE be.level < 10) SELECT be.component_id, dp.product_name, SUM(be.quantity) as total_quantity, dp.unit_cost, SUM(be.quantity) * dp.unit_cost as extended_cost FROM bom_explosion be JOIN dim_product dp ON be.component_id = dp.product_id GROUP BY be.component_id, dp.product_name, dp.unit_cost ORDER BY extended_cost DESC",
      "agent_sql": "WITH RECURSIVE bom_explosion AS (SELECT parent_product_id, component_id, quantity, 1 as level FROM bill_of_materials WHERE parent_product_id = 'PROD001' UNION ALL SELECT bom.parent_product_id, bom.component_id, bom.quantity * be.quantity, be.level + 1 FROM bill_of_materials bom JOIN bom_explosion be ON bom.parent_product_id = be.component_id WHERE be.level < 10) SELECT be.component_id, dp.product_name, SUM(be.quantity) as total_quantity, dp.unit_cost, SUM(be.quantity) * dp.unit_cost as extended_cost FROM bom_explosion be JOIN dim_product dp ON be.component_id = dp.product_id GROUP BY be.component_id, dp.product_name, dp.unit_cost ORDER BY extended_cost DESC",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.08082389831542969,
      "rows_returned": 6,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_market_basket",
      "question": "Market basket analysis: find products frequently bought together",
      "difficulty": "enterprise",
      "tags": [
        "market_basket",
        "association",
        "retail",
        "analytics"
      ],
      "gold_sql": "WITH transaction_products AS (SELECT transaction_id, product_id FROM sales_fact GROUP BY transaction_id, product_id), product_pairs AS (SELECT t1.product_id as product_a, t2.product_id as product_b, COUNT(DISTINCT t1.transaction_id) as times_bought_together FROM transaction_products t1 JOIN transaction_products t2 ON t1.transaction_id = t2.transaction_id AND t1.product_id < t2.product_id GROUP BY t1.product_id, t2.product_id), product_counts AS (SELECT product_id, COUNT(DISTINCT transaction_id) as times_bought FROM transaction_products GROUP BY product_id) SELECT dp1.product_name as product_a, dp2.product_name as product_b, pp.times_bought_together, ROUND(pp.times_bought_together * 100.0 / pc1.times_bought, 2) as pct_of_a_transactions, ROUND(pp.times_bought_together * 100.0 / pc2.times_bought, 2) as pct_of_b_transactions FROM product_pairs pp JOIN dim_product dp1 ON pp.product_a = dp1.product_id JOIN dim_product dp2 ON pp.product_b = dp2.product_id JOIN product_counts pc1 ON pp.product_a = pc1.product_id JOIN product_counts pc2 ON pp.product_b = pc2.product_id WHERE pp.times_bought_together >= 5 ORDER BY pp.times_bought_together DESC LIMIT 20",
      "agent_sql": "WITH transaction_products AS (SELECT transaction_id, product_id FROM sales_fact GROUP BY transaction_id, product_id), product_pairs AS (SELECT t1.product_id as product_a, t2.product_id as product_b, COUNT(DISTINCT t1.transaction_id) as times_bought_together FROM transaction_products t1 JOIN transaction_products t2 ON t1.transaction_id = t2.transaction_id AND t1.product_id < t2.product_id GROUP BY t1.product_id, t2.product_id), product_counts AS (SELECT product_id, COUNT(DISTINCT transaction_id) as times_bought FROM transaction_products GROUP BY product_id) SELECT dp1.product_name as product_a, dp2.product_name as product_b, pp.times_bought_together, ROUND(pp.times_bought_together * 100.0 / pc1.times_bought, 2) as pct_of_a_transactions, ROUND(pp.times_bought_together * 100.0 / pc2.times_bought, 2) as pct_of_b_transactions FROM product_pairs pp JOIN dim_product dp1 ON pp.product_a = dp1.product_id JOIN dim_product dp2 ON pp.product_b = dp2.product_id JOIN product_counts pc1 ON pp.product_a = pc1.product_id JOIN product_counts pc2 ON pp.product_b = pc2.product_id WHERE pp.times_bought_together >= 5 ORDER BY pp.times_bought_together DESC LIMIT 20",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.40793418884277344,
      "rows_returned": 0,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_churn_prediction",
      "question": "Identify customers at risk of churning based on declining purchase frequency",
      "difficulty": "enterprise",
      "tags": [
        "churn",
        "prediction",
        "customer_analytics",
        "retention"
      ],
      "gold_sql": "WITH monthly_activity AS (SELECT customer_id, strftime('%Y-%m', order_date) as month, COUNT(*) as orders, SUM(total_amount) as revenue FROM orders_fact GROUP BY customer_id, strftime('%Y-%m', order_date)), customer_trends AS (SELECT customer_id, month, orders, revenue, LAG(orders, 1) OVER (PARTITION BY customer_id ORDER BY month) as prev_orders, LAG(orders, 2) OVER (PARTITION BY customer_id ORDER BY month) as prev2_orders, LAG(orders, 3) OVER (PARTITION BY customer_id ORDER BY month) as prev3_orders FROM monthly_activity), churn_risk AS (SELECT customer_id, MAX(month) as last_active_month, AVG(orders) as avg_monthly_orders, CASE WHEN MAX(month) < strftime('%Y-%m', 'now', '-2 months') THEN 'Churned' WHEN MAX(orders) < AVG(orders) * 0.5 THEN 'High Risk' WHEN MAX(orders) < AVG(orders) * 0.75 THEN 'Medium Risk' ELSE 'Low Risk' END as churn_status FROM customer_trends GROUP BY customer_id) SELECT dc.customer_name, cr.last_active_month, ROUND(cr.avg_monthly_orders, 2) as avg_monthly_orders, cr.churn_status FROM churn_risk cr JOIN dim_customer dc ON cr.customer_id = dc.customer_id WHERE cr.churn_status IN ('High Risk', 'Churned') ORDER BY cr.churn_status, cr.last_active_month",
      "agent_sql": "WITH monthly_activity AS (SELECT customer_id, strftime('%Y-%m', order_date) as month, COUNT(*) as orders, SUM(total_amount) as revenue FROM orders_fact GROUP BY customer_id, strftime('%Y-%m', order_date)), customer_trends AS (SELECT customer_id, month, orders, revenue, LAG(orders, 1) OVER (PARTITION BY customer_id ORDER BY month) as prev_orders, LAG(orders, 2) OVER (PARTITION BY customer_id ORDER BY month) as prev2_orders, LAG(orders, 3) OVER (PARTITION BY customer_id ORDER BY month) as prev3_orders FROM monthly_activity), churn_risk AS (SELECT customer_id, MAX(month) as last_active_month, AVG(orders) as avg_monthly_orders, CASE WHEN MAX(month) < strftime('%Y-%m', 'now', '-2 months') THEN 'Churned' WHEN MAX(orders) < AVG(orders) * 0.5 THEN 'High Risk' WHEN MAX(orders) < AVG(orders) * 0.75 THEN 'Medium Risk' ELSE 'Low Risk' END as churn_status FROM customer_trends GROUP BY customer_id) SELECT dc.customer_name, cr.last_active_month, ROUND(cr.avg_monthly_orders, 2) as avg_monthly_orders, cr.churn_status FROM churn_risk cr JOIN dim_customer dc ON cr.customer_id = dc.customer_id WHERE cr.churn_status IN ('High Risk', 'Churned') ORDER BY cr.churn_status, cr.last_active_month",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.5970001220703125,
      "rows_returned": 50,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_inventory_abc",
      "question": "ABC inventory classification based on revenue contribution (Pareto analysis)",
      "difficulty": "enterprise",
      "tags": [
        "abc_analysis",
        "inventory",
        "pareto",
        "classification"
      ],
      "gold_sql": "WITH product_revenue AS (SELECT dp.product_id, dp.product_name, SUM(sf.quantity * sf.unit_price) as revenue FROM sales_fact sf JOIN dim_product dp ON sf.product_id = dp.product_id GROUP BY dp.product_id, dp.product_name), ranked_products AS (SELECT product_id, product_name, revenue, SUM(revenue) OVER (ORDER BY revenue DESC) as cumulative_revenue, SUM(revenue) OVER () as total_revenue FROM product_revenue) SELECT product_id, product_name, revenue, ROUND(revenue * 100.0 / total_revenue, 2) as pct_of_total, ROUND(cumulative_revenue * 100.0 / total_revenue, 2) as cumulative_pct, CASE WHEN cumulative_revenue <= total_revenue * 0.8 THEN 'A' WHEN cumulative_revenue <= total_revenue * 0.95 THEN 'B' ELSE 'C' END as abc_class FROM ranked_products ORDER BY revenue DESC",
      "agent_sql": "WITH product_revenue AS (SELECT dp.product_id, dp.product_name, SUM(sf.quantity * sf.unit_price) as revenue FROM sales_fact sf JOIN dim_product dp ON sf.product_id = dp.product_id GROUP BY dp.product_id, dp.product_name), ranked_products AS (SELECT product_id, product_name, revenue, SUM(revenue) OVER (ORDER BY revenue DESC) as cumulative_revenue, SUM(revenue) OVER () as total_revenue FROM product_revenue) SELECT product_id, product_name, revenue, ROUND(revenue * 100.0 / total_revenue, 2) as pct_of_total, ROUND(cumulative_revenue * 100.0 / total_revenue, 2) as cumulative_pct, CASE WHEN cumulative_revenue <= total_revenue * 0.8 THEN 'A' WHEN cumulative_revenue <= total_revenue * 0.95 THEN 'B' ELSE 'C' END as abc_class FROM ranked_products ORDER BY revenue DESC",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.33593177795410156,
      "rows_returned": 30,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Consider adding a WHERE clause"
      ]
    },
    {
      "task_id": "ent_cohort_retention",
      "question": "Calculate monthly retention rates by customer acquisition cohort",
      "difficulty": "enterprise",
      "tags": [
        "cohort",
        "retention",
        "customer_analytics",
        "growth"
      ],
      "gold_sql": "WITH first_purchase AS (SELECT customer_id, MIN(DATE(order_date)) as cohort_date, strftime('%Y-%m', MIN(order_date)) as cohort_month FROM orders_fact GROUP BY customer_id), monthly_activity AS (SELECT o.customer_id, fp.cohort_month, strftime('%Y-%m', o.order_date) as activity_month, (strftime('%Y', o.order_date) - strftime('%Y', fp.cohort_date)) * 12 + (strftime('%m', o.order_date) - strftime('%m', fp.cohort_date)) as months_since_first FROM orders_fact o JOIN first_purchase fp ON o.customer_id = fp.customer_id), cohort_sizes AS (SELECT cohort_month, COUNT(DISTINCT customer_id) as cohort_size FROM first_purchase GROUP BY cohort_month), retention AS (SELECT ma.cohort_month, ma.months_since_first, COUNT(DISTINCT ma.customer_id) as active_customers FROM monthly_activity ma GROUP BY ma.cohort_month, ma.months_since_first) SELECT r.cohort_month, cs.cohort_size, r.months_since_first, r.active_customers, ROUND(r.active_customers * 100.0 / cs.cohort_size, 2) as retention_rate FROM retention r JOIN cohort_sizes cs ON r.cohort_month = cs.cohort_month WHERE r.months_since_first <= 12 ORDER BY r.cohort_month, r.months_since_first",
      "agent_sql": "WITH first_purchase AS (SELECT customer_id, MIN(DATE(order_date)) as cohort_date, strftime('%Y-%m', MIN(order_date)) as cohort_month FROM orders_fact GROUP BY customer_id), monthly_activity AS (SELECT o.customer_id, fp.cohort_month, strftime('%Y-%m', o.order_date) as activity_month, (strftime('%Y', o.order_date) - strftime('%Y', fp.cohort_date)) * 12 + (strftime('%m', o.order_date) - strftime('%m', fp.cohort_date)) as months_since_first FROM orders_fact o JOIN first_purchase fp ON o.customer_id = fp.customer_id), cohort_sizes AS (SELECT cohort_month, COUNT(DISTINCT customer_id) as cohort_size FROM first_purchase GROUP BY cohort_month), retention AS (SELECT ma.cohort_month, ma.months_since_first, COUNT(DISTINCT ma.customer_id) as active_customers FROM monthly_activity ma GROUP BY ma.cohort_month, ma.months_since_first) SELECT r.cohort_month, cs.cohort_size, r.months_since_first, r.active_customers, ROUND(r.active_customers * 100.0 / cs.cohort_size, 2) as retention_rate FROM retention r JOIN cohort_sizes cs ON r.cohort_month = cs.cohort_month WHERE r.months_since_first <= 12 ORDER BY r.cohort_month, r.months_since_first",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.67901611328125,
      "rows_returned": 40,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_pivot_monthly_sales",
      "question": "Pivot monthly sales data by category (columns for each month)",
      "difficulty": "enterprise",
      "tags": [
        "pivot",
        "crosstab",
        "reporting",
        "aggregation"
      ],
      "gold_sql": "SELECT dp.category, SUM(CASE WHEN strftime('%m', dd.full_date) = '01' THEN sf.quantity * sf.unit_price ELSE 0 END) as jan_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '02' THEN sf.quantity * sf.unit_price ELSE 0 END) as feb_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '03' THEN sf.quantity * sf.unit_price ELSE 0 END) as mar_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '04' THEN sf.quantity * sf.unit_price ELSE 0 END) as apr_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '05' THEN sf.quantity * sf.unit_price ELSE 0 END) as may_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '06' THEN sf.quantity * sf.unit_price ELSE 0 END) as jun_sales, SUM(sf.quantity * sf.unit_price) as total_h1 FROM sales_fact sf JOIN dim_product dp ON sf.product_id = dp.product_id JOIN dim_date dd ON sf.date_id = dd.date_id WHERE dd.year = 2024 AND dd.month <= 6 GROUP BY dp.category ORDER BY total_h1 DESC",
      "agent_sql": "SELECT dp.category, SUM(CASE WHEN strftime('%m', dd.full_date) = '01' THEN sf.quantity * sf.unit_price ELSE 0 END) as jan_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '02' THEN sf.quantity * sf.unit_price ELSE 0 END) as feb_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '03' THEN sf.quantity * sf.unit_price ELSE 0 END) as mar_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '04' THEN sf.quantity * sf.unit_price ELSE 0 END) as apr_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '05' THEN sf.quantity * sf.unit_price ELSE 0 END) as may_sales, SUM(CASE WHEN strftime('%m', dd.full_date) = '06' THEN sf.quantity * sf.unit_price ELSE 0 END) as jun_sales, SUM(sf.quantity * sf.unit_price) as total_h1 FROM sales_fact sf JOIN dim_product dp ON sf.product_id = dp.product_id JOIN dim_date dd ON sf.date_id = dd.date_id WHERE dd.year = 2024 AND dd.month <= 6 GROUP BY dp.category ORDER BY total_h1 DESC",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.4200935363769531,
      "rows_returned": 5,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_anomaly_detection",
      "question": "Detect sales anomalies using statistical thresholds (3 standard deviations)",
      "difficulty": "enterprise",
      "tags": [
        "anomaly",
        "statistics",
        "data_quality",
        "monitoring"
      ],
      "gold_sql": "WITH daily_sales AS (SELECT dd.full_date, SUM(sf.quantity * sf.unit_price) as sales FROM sales_fact sf JOIN dim_date dd ON sf.date_id = dd.date_id GROUP BY dd.full_date), stats AS (SELECT AVG(sales) as mean_sales, AVG(sales * sales) - AVG(sales) * AVG(sales) as variance FROM daily_sales), anomalies AS (SELECT ds.full_date, ds.sales, s.mean_sales, SQRT(s.variance) as std_dev, (ds.sales - s.mean_sales) / NULLIF(SQRT(s.variance), 0) as z_score FROM daily_sales ds CROSS JOIN stats s) SELECT full_date, ROUND(sales, 2) as sales, ROUND(mean_sales, 2) as mean_sales, ROUND(std_dev, 2) as std_dev, ROUND(z_score, 2) as z_score, CASE WHEN z_score > 3 THEN 'High Anomaly' WHEN z_score > 2 THEN 'Moderate High' WHEN z_score < -3 THEN 'Low Anomaly' WHEN z_score < -2 THEN 'Moderate Low' ELSE 'Normal' END as anomaly_type FROM anomalies WHERE ABS(z_score) > 2 ORDER BY ABS(z_score) DESC",
      "agent_sql": "WITH daily_sales AS (SELECT dd.full_date, SUM(sf.quantity * sf.unit_price) as sales FROM sales_fact sf JOIN dim_date dd ON sf.date_id = dd.date_id GROUP BY dd.full_date), stats AS (SELECT AVG(sales) as mean_sales, AVG(sales * sales) - AVG(sales) * AVG(sales) as variance FROM daily_sales), anomalies AS (SELECT ds.full_date, ds.sales, s.mean_sales, SQRT(s.variance) as std_dev, (ds.sales - s.mean_sales) / NULLIF(SQRT(s.variance), 0) as z_score FROM daily_sales ds CROSS JOIN stats s) SELECT full_date, ROUND(sales, 2) as sales, ROUND(mean_sales, 2) as mean_sales, ROUND(std_dev, 2) as std_dev, ROUND(z_score, 2) as z_score, CASE WHEN z_score > 3 THEN 'High Anomaly' WHEN z_score > 2 THEN 'Moderate High' WHEN z_score < -3 THEN 'Low Anomaly' WHEN z_score < -2 THEN 'Moderate Low' ELSE 'Normal' END as anomaly_type FROM anomalies WHERE ABS(z_score) > 2 ORDER BY ABS(z_score) DESC",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.2899169921875,
      "rows_returned": 13,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_data_quality_check",
      "question": "Comprehensive data quality report: nulls, duplicates, orphans, and ranges",
      "difficulty": "enterprise",
      "tags": [
        "data_quality",
        "validation",
        "etl",
        "monitoring"
      ],
      "gold_sql": "WITH null_checks AS (SELECT 'orders_fact' as table_name, 'customer_id' as column_name, COUNT(*) as total_rows, SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_count FROM orders_fact UNION ALL SELECT 'orders_fact', 'product_id', COUNT(*), SUM(CASE WHEN product_id IS NULL THEN 1 ELSE 0 END) FROM orders_fact UNION ALL SELECT 'orders_fact', 'total_amount', COUNT(*), SUM(CASE WHEN total_amount IS NULL THEN 1 ELSE 0 END) FROM orders_fact), orphan_checks AS (SELECT 'orders_fact' as table_name, 'customer_id' as fk_column, COUNT(*) as orphan_count FROM orders_fact o WHERE NOT EXISTS (SELECT 1 FROM dim_customer c WHERE o.customer_id = c.customer_id) UNION ALL SELECT 'orders_fact', 'product_id', COUNT(*) FROM orders_fact o WHERE NOT EXISTS (SELECT 1 FROM dim_product p WHERE o.product_id = p.product_id)), duplicate_checks AS (SELECT 'orders_fact' as table_name, 'order_id' as key_column, COUNT(*) - COUNT(DISTINCT order_id) as duplicate_count FROM orders_fact) SELECT 'Null Check' as check_type, table_name, column_name as detail, total_rows, null_count as issue_count, ROUND(null_count * 100.0 / total_rows, 2) as issue_pct FROM null_checks WHERE null_count > 0 UNION ALL SELECT 'Orphan Check', table_name, fk_column, NULL, orphan_count, NULL FROM orphan_checks WHERE orphan_count > 0 UNION ALL SELECT 'Duplicate Check', table_name, key_column, NULL, duplicate_count, NULL FROM duplicate_checks WHERE duplicate_count > 0 ORDER BY check_type, table_name",
      "agent_sql": "WITH null_checks AS (SELECT 'orders_fact' as table_name, 'customer_id' as column_name, COUNT(*) as total_rows, SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_count FROM orders_fact UNION ALL SELECT 'orders_fact', 'product_id', COUNT(*), SUM(CASE WHEN product_id IS NULL THEN 1 ELSE 0 END) FROM orders_fact UNION ALL SELECT 'orders_fact', 'total_amount', COUNT(*), SUM(CASE WHEN total_amount IS NULL THEN 1 ELSE 0 END) FROM orders_fact), orphan_checks AS (SELECT 'orders_fact' as table_name, 'customer_id' as fk_column, COUNT(*) as orphan_count FROM orders_fact o WHERE NOT EXISTS (SELECT 1 FROM dim_customer c WHERE o.customer_id = c.customer_id) UNION ALL SELECT 'orders_fact', 'product_id', COUNT(*) FROM orders_fact o WHERE NOT EXISTS (SELECT 1 FROM dim_product p WHERE o.product_id = p.product_id)), duplicate_checks AS (SELECT 'orders_fact' as table_name, 'order_id' as key_column, COUNT(*) - COUNT(DISTINCT order_id) as duplicate_count FROM orders_fact) SELECT 'Null Check' as check_type, table_name, column_name as detail, total_rows, null_count as issue_count, ROUND(null_count * 100.0 / total_rows, 2) as issue_pct FROM null_checks WHERE null_count > 0 UNION ALL SELECT 'Orphan Check', table_name, fk_column, NULL, orphan_count, NULL FROM orphan_checks WHERE orphan_count > 0 UNION ALL SELECT 'Duplicate Check', table_name, key_column, NULL, duplicate_count, NULL FROM duplicate_checks WHERE duplicate_count > 0 ORDER BY check_type, table_name",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.22077560424804688,
      "rows_returned": 0,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_late_arriving_facts",
      "question": "Handle late-arriving facts: find and flag records loaded after expected time",
      "difficulty": "enterprise",
      "tags": [
        "etl",
        "data_quality",
        "late_arriving",
        "monitoring"
      ],
      "gold_sql": "WITH expected_load AS (SELECT sf.transaction_id, sf.order_date, sf.load_timestamp, DATE(sf.order_date, '+1 day') as expected_load_date, CASE WHEN DATE(sf.load_timestamp) > DATE(sf.order_date, '+1 day') THEN 1 ELSE 0 END as is_late, julianday(sf.load_timestamp) - julianday(sf.order_date) as days_delay FROM sales_fact sf) SELECT strftime('%Y-%m', order_date) as order_month, COUNT(*) as total_records, SUM(is_late) as late_records, ROUND(SUM(is_late) * 100.0 / COUNT(*), 2) as late_pct, ROUND(AVG(CASE WHEN is_late = 1 THEN days_delay ELSE NULL END), 2) as avg_delay_days, MAX(CASE WHEN is_late = 1 THEN days_delay ELSE NULL END) as max_delay_days FROM expected_load GROUP BY strftime('%Y-%m', order_date) ORDER BY order_month",
      "agent_sql": "WITH expected_load AS (SELECT sf.transaction_id, sf.order_date, sf.load_timestamp, DATE(sf.order_date, '+1 day') as expected_load_date, CASE WHEN DATE(sf.load_timestamp) > DATE(sf.order_date, '+1 day') THEN 1 ELSE 0 END as is_late, julianday(sf.load_timestamp) - julianday(sf.order_date) as days_delay FROM sales_fact sf) SELECT strftime('%Y-%m', order_date) as order_month, COUNT(*) as total_records, SUM(is_late) as late_records, ROUND(SUM(is_late) * 100.0 / COUNT(*), 2) as late_pct, ROUND(AVG(CASE WHEN is_late = 1 THEN days_delay ELSE NULL END), 2) as avg_delay_days, MAX(CASE WHEN is_late = 1 THEN days_delay ELSE NULL END) as max_delay_days FROM expected_load GROUP BY strftime('%Y-%m', order_date) ORDER BY order_month",
      "status": "failed",
      "overall_score": 0.0,
      "correctness": 0.0,
      "efficiency": 0.0,
      "safety": 0.0,
      "completeness": 0.0,
      "semantic_accuracy": 0.0,
      "best_practices": 0.0,
      "plan_quality": 0.0,
      "execution_time_ms": 0.0,
      "rows_returned": 0,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": "no such column: sf.order_date",
      "suggestions": []
    },
    {
      "task_id": "ent_slowly_changing_merge",
      "question": "Simulate SCD Type 2 merge operation for dimension update",
      "difficulty": "enterprise",
      "tags": [
        "scd",
        "merge",
        "etl",
        "dimension"
      ],
      "gold_sql": "WITH current_records AS (SELECT * FROM dim_customer_scd WHERE is_current = 1), staged_updates AS (SELECT * FROM staging_customer), changes AS (SELECT s.customer_id, s.customer_name, s.email, s.segment, c.customer_name as old_name, c.segment as old_segment, CASE WHEN c.customer_id IS NULL THEN 'INSERT' WHEN s.customer_name != c.customer_name OR s.segment != c.segment THEN 'UPDATE' ELSE 'NO_CHANGE' END as change_type FROM staged_updates s LEFT JOIN current_records c ON s.customer_id = c.customer_id) SELECT change_type, COUNT(*) as record_count, GROUP_CONCAT(customer_id) as affected_customers FROM changes GROUP BY change_type ORDER BY CASE change_type WHEN 'INSERT' THEN 1 WHEN 'UPDATE' THEN 2 ELSE 3 END",
      "agent_sql": "WITH current_records AS (SELECT * FROM dim_customer_scd WHERE is_current = 1), staged_updates AS (SELECT * FROM staging_customer), changes AS (SELECT s.customer_id, s.customer_name, s.email, s.segment, c.customer_name as old_name, c.segment as old_segment, CASE WHEN c.customer_id IS NULL THEN 'INSERT' WHEN s.customer_name != c.customer_name OR s.segment != c.segment THEN 'UPDATE' ELSE 'NO_CHANGE' END as change_type FROM staged_updates s LEFT JOIN current_records c ON s.customer_id = c.customer_id) SELECT change_type, COUNT(*) as record_count, GROUP_CONCAT(customer_id) as affected_customers FROM changes GROUP BY change_type ORDER BY CASE change_type WHEN 'INSERT' THEN 1 WHEN 'UPDATE' THEN 2 ELSE 3 END",
      "status": "success",
      "overall_score": 0.995,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.9,
      "plan_quality": 1.0,
      "execution_time_ms": 0.09870529174804688,
      "rows_returned": 2,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": [
        "Specify only the columns you need"
      ]
    },
    {
      "task_id": "ent_financial_reporting",
      "question": "Generate financial summary with period comparisons and variances",
      "difficulty": "enterprise",
      "tags": [
        "financial",
        "reporting",
        "variance",
        "month_over_month"
      ],
      "gold_sql": "WITH period_totals AS (SELECT strftime('%Y', order_date) as year, strftime('%m', order_date) as month, SUM(total_amount) as revenue, SUM(cost_amount) as cost, SUM(total_amount) - SUM(cost_amount) as gross_profit, COUNT(DISTINCT order_id) as transactions, COUNT(DISTINCT customer_id) as unique_customers FROM orders_fact GROUP BY strftime('%Y', order_date), strftime('%m', order_date)) SELECT curr.year, curr.month, curr.revenue, prev.revenue as prev_month_revenue, ROUND((curr.revenue - COALESCE(prev.revenue, 0)) * 100.0 / NULLIF(prev.revenue, 0), 2) as revenue_growth_pct, curr.gross_profit, ROUND(curr.gross_profit * 100.0 / curr.revenue, 2) as gross_margin_pct, curr.transactions, ROUND(curr.revenue / curr.transactions, 2) as avg_transaction_value, curr.unique_customers FROM period_totals curr LEFT JOIN period_totals prev ON (curr.year = prev.year AND curr.month = printf('%02d', CAST(prev.month AS INTEGER) + 1)) OR (curr.month = '01' AND prev.month = '12' AND curr.year = printf('%04d', CAST(prev.year AS INTEGER) + 1)) ORDER BY curr.year, curr.month",
      "agent_sql": "WITH period_totals AS (SELECT strftime('%Y', order_date) as year, strftime('%m', order_date) as month, SUM(total_amount) as revenue, SUM(cost_amount) as cost, SUM(total_amount) - SUM(cost_amount) as gross_profit, COUNT(DISTINCT order_id) as transactions, COUNT(DISTINCT customer_id) as unique_customers FROM orders_fact GROUP BY strftime('%Y', order_date), strftime('%m', order_date)) SELECT curr.year, curr.month, curr.revenue, prev.revenue as prev_month_revenue, ROUND((curr.revenue - COALESCE(prev.revenue, 0)) * 100.0 / NULLIF(prev.revenue, 0), 2) as revenue_growth_pct, curr.gross_profit, ROUND(curr.gross_profit * 100.0 / curr.revenue, 2) as gross_margin_pct, curr.transactions, ROUND(curr.revenue / curr.transactions, 2) as avg_transaction_value, curr.unique_customers FROM period_totals curr LEFT JOIN period_totals prev ON (curr.year = prev.year AND curr.month = printf('%02d', CAST(prev.month AS INTEGER) + 1)) OR (curr.month = '01' AND prev.month = '12' AND curr.year = printf('%04d', CAST(prev.year AS INTEGER) + 1)) ORDER BY curr.year, curr.month",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 0.32401084899902344,
      "rows_returned": 10,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_customer_360",
      "question": "Build a customer 360 view combining transactions, support, and engagement data",
      "difficulty": "enterprise",
      "tags": [
        "customer_360",
        "integration",
        "analytics",
        "crm"
      ],
      "gold_sql": "WITH order_summary AS (SELECT customer_id, COUNT(*) as total_orders, SUM(total_amount) as lifetime_value, MIN(order_date) as first_order, MAX(order_date) as last_order, AVG(total_amount) as avg_order_value FROM orders_fact GROUP BY customer_id), support_summary AS (SELECT customer_id, COUNT(*) as total_tickets, SUM(CASE WHEN status = 'open' THEN 1 ELSE 0 END) as open_tickets, AVG(resolution_hours) as avg_resolution_time FROM support_tickets GROUP BY customer_id), engagement_summary AS (SELECT customer_id, MAX(last_login) as last_login, SUM(page_views) as total_page_views, SUM(email_opens) as email_opens, SUM(email_clicks) as email_clicks FROM customer_engagement GROUP BY customer_id) SELECT dc.customer_id, dc.customer_name, dc.email, dc.segment, os.total_orders, os.lifetime_value, os.first_order, os.last_order, os.avg_order_value, COALESCE(ss.total_tickets, 0) as support_tickets, COALESCE(ss.open_tickets, 0) as open_tickets, ss.avg_resolution_time, es.last_login, COALESCE(es.total_page_views, 0) as page_views, ROUND(COALESCE(es.email_clicks, 0) * 100.0 / NULLIF(es.email_opens, 0), 2) as email_click_rate FROM dim_customer dc LEFT JOIN order_summary os ON dc.customer_id = os.customer_id LEFT JOIN support_summary ss ON dc.customer_id = ss.customer_id LEFT JOIN engagement_summary es ON dc.customer_id = es.customer_id ORDER BY os.lifetime_value DESC NULLS LAST",
      "agent_sql": "WITH order_summary AS (SELECT customer_id, COUNT(*) as total_orders, SUM(total_amount) as lifetime_value, MIN(order_date) as first_order, MAX(order_date) as last_order, AVG(total_amount) as avg_order_value FROM orders_fact GROUP BY customer_id), support_summary AS (SELECT customer_id, COUNT(*) as total_tickets, SUM(CASE WHEN status = 'open' THEN 1 ELSE 0 END) as open_tickets, AVG(resolution_hours) as avg_resolution_time FROM support_tickets GROUP BY customer_id), engagement_summary AS (SELECT customer_id, MAX(last_login) as last_login, SUM(page_views) as total_page_views, SUM(email_opens) as email_opens, SUM(email_clicks) as email_clicks FROM customer_engagement GROUP BY customer_id) SELECT dc.customer_id, dc.customer_name, dc.email, dc.segment, os.total_orders, os.lifetime_value, os.first_order, os.last_order, os.avg_order_value, COALESCE(ss.total_tickets, 0) as support_tickets, COALESCE(ss.open_tickets, 0) as open_tickets, ss.avg_resolution_time, es.last_login, COALESCE(es.total_page_views, 0) as page_views, ROUND(COALESCE(es.email_clicks, 0) * 100.0 / NULLIF(es.email_opens, 0), 2) as email_click_rate FROM dim_customer dc LEFT JOIN order_summary os ON dc.customer_id = os.customer_id LEFT JOIN support_summary ss ON dc.customer_id = ss.customer_id LEFT JOIN engagement_summary es ON dc.customer_id = es.customer_id ORDER BY os.lifetime_value DESC NULLS LAST",
      "status": "success",
      "overall_score": 1.0,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 1.0,
      "plan_quality": 1.0,
      "execution_time_ms": 0.4239082336425781,
      "rows_returned": 50,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_attribution_model",
      "question": "Multi-touch attribution model for marketing channels",
      "difficulty": "enterprise",
      "tags": [
        "attribution",
        "marketing",
        "analytics",
        "multi_touch"
      ],
      "gold_sql": "WITH touchpoints AS (SELECT customer_id, order_id, channel, touch_timestamp, ROW_NUMBER() OVER (PARTITION BY customer_id, order_id ORDER BY touch_timestamp) as touch_order, COUNT(*) OVER (PARTITION BY customer_id, order_id) as total_touches FROM marketing_touches), attributed AS (SELECT t.customer_id, t.order_id, t.channel, t.touch_order, t.total_touches, o.total_amount, CASE WHEN t.touch_order = 1 THEN o.total_amount ELSE 0 END as first_touch_value, CASE WHEN t.touch_order = t.total_touches THEN o.total_amount ELSE 0 END as last_touch_value, o.total_amount * 1.0 / t.total_touches as linear_value, CASE WHEN t.touch_order = 1 THEN o.total_amount * 0.4 WHEN t.touch_order = t.total_touches THEN o.total_amount * 0.4 ELSE o.total_amount * 0.2 / (t.total_touches - 2) END as position_based_value FROM touchpoints t JOIN orders_fact o ON t.order_id = o.order_id) SELECT channel, COUNT(DISTINCT order_id) as conversions, SUM(first_touch_value) as first_touch_revenue, SUM(last_touch_value) as last_touch_revenue, SUM(linear_value) as linear_revenue, SUM(position_based_value) as position_based_revenue FROM attributed GROUP BY channel ORDER BY position_based_revenue DESC",
      "agent_sql": "WITH touchpoints AS (SELECT customer_id, order_id, channel, touch_timestamp, ROW_NUMBER() OVER (PARTITION BY customer_id, order_id ORDER BY touch_timestamp) as touch_order, COUNT(*) OVER (PARTITION BY customer_id, order_id) as total_touches FROM marketing_touches), attributed AS (SELECT t.customer_id, t.order_id, t.channel, t.touch_order, t.total_touches, o.total_amount, CASE WHEN t.touch_order = 1 THEN o.total_amount ELSE 0 END as first_touch_value, CASE WHEN t.touch_order = t.total_touches THEN o.total_amount ELSE 0 END as last_touch_value, o.total_amount * 1.0 / t.total_touches as linear_value, CASE WHEN t.touch_order = 1 THEN o.total_amount * 0.4 WHEN t.touch_order = t.total_touches THEN o.total_amount * 0.4 ELSE o.total_amount * 0.2 / (t.total_touches - 2) END as position_based_value FROM touchpoints t JOIN orders_fact o ON t.order_id = o.order_id) SELECT channel, COUNT(DISTINCT order_id) as conversions, SUM(first_touch_value) as first_touch_revenue, SUM(last_touch_value) as last_touch_revenue, SUM(linear_value) as linear_revenue, SUM(position_based_value) as position_based_revenue FROM attributed GROUP BY channel ORDER BY position_based_revenue DESC",
      "status": "success",
      "overall_score": 0.9974999999999999,
      "correctness": 1.0,
      "efficiency": 1.0,
      "safety": 1.0,
      "completeness": 1.0,
      "semantic_accuracy": 1.0,
      "best_practices": 0.95,
      "plan_quality": 1.0,
      "execution_time_ms": 1.1601448059082031,
      "rows_returned": 6,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": null,
      "suggestions": []
    },
    {
      "task_id": "ent_inventory_forecast",
      "question": "Inventory demand forecasting based on historical sales and seasonality",
      "difficulty": "enterprise",
      "tags": [
        "inventory",
        "forecasting",
        "supply_chain",
        "statistics"
      ],
      "gold_sql": "WITH daily_demand AS (SELECT product_id, DATE(order_date) as sale_date, SUM(quantity) as daily_quantity FROM sales_fact GROUP BY product_id, DATE(order_date)), product_stats AS (SELECT product_id, AVG(daily_quantity) as avg_daily_demand, AVG(daily_quantity * daily_quantity) - AVG(daily_quantity) * AVG(daily_quantity) as variance, COUNT(DISTINCT sale_date) as days_with_sales, (julianday(MAX(sale_date)) - julianday(MIN(sale_date))) as date_range FROM daily_demand GROUP BY product_id), safety_stock AS (SELECT ps.product_id, dp.product_name, ps.avg_daily_demand, SQRT(ps.variance) as std_dev, ps.days_with_sales, ROUND(ps.avg_daily_demand * 7, 0) as weekly_forecast, ROUND(1.65 * SQRT(ps.variance) * SQRT(7), 0) as safety_stock_7day, i.current_stock, ROUND((i.current_stock - 1.65 * SQRT(ps.variance) * SQRT(7)) / NULLIF(ps.avg_daily_demand, 0), 0) as days_of_supply FROM product_stats ps JOIN dim_product dp ON ps.product_id = dp.product_id LEFT JOIN inventory i ON ps.product_id = i.product_id) SELECT product_id, product_name, ROUND(avg_daily_demand, 2) as avg_daily_demand, weekly_forecast, safety_stock_7day, COALESCE(current_stock, 0) as current_stock, days_of_supply, CASE WHEN days_of_supply < 7 THEN 'Critical' WHEN days_of_supply < 14 THEN 'Low' WHEN days_of_supply < 30 THEN 'Adequate' ELSE 'Excess' END as stock_status FROM safety_stock ORDER BY days_of_supply NULLS FIRST",
      "agent_sql": "WITH daily_demand AS (SELECT product_id, DATE(order_date) as sale_date, SUM(quantity) as daily_quantity FROM sales_fact GROUP BY product_id, DATE(order_date)), product_stats AS (SELECT product_id, AVG(daily_quantity) as avg_daily_demand, AVG(daily_quantity * daily_quantity) - AVG(daily_quantity) * AVG(daily_quantity) as variance, COUNT(DISTINCT sale_date) as days_with_sales, (julianday(MAX(sale_date)) - julianday(MIN(sale_date))) as date_range FROM daily_demand GROUP BY product_id), safety_stock AS (SELECT ps.product_id, dp.product_name, ps.avg_daily_demand, SQRT(ps.variance) as std_dev, ps.days_with_sales, ROUND(ps.avg_daily_demand * 7, 0) as weekly_forecast, ROUND(1.65 * SQRT(ps.variance) * SQRT(7), 0) as safety_stock_7day, i.current_stock, ROUND((i.current_stock - 1.65 * SQRT(ps.variance) * SQRT(7)) / NULLIF(ps.avg_daily_demand, 0), 0) as days_of_supply FROM product_stats ps JOIN dim_product dp ON ps.product_id = dp.product_id LEFT JOIN inventory i ON ps.product_id = i.product_id) SELECT product_id, product_name, ROUND(avg_daily_demand, 2) as avg_daily_demand, weekly_forecast, safety_stock_7day, COALESCE(current_stock, 0) as current_stock, days_of_supply, CASE WHEN days_of_supply < 7 THEN 'Critical' WHEN days_of_supply < 14 THEN 'Low' WHEN days_of_supply < 30 THEN 'Adequate' ELSE 'Excess' END as stock_status FROM safety_stock ORDER BY days_of_supply NULLS FIRST",
      "status": "failed",
      "overall_score": 0.0,
      "correctness": 0.0,
      "efficiency": 0.0,
      "safety": 0.0,
      "completeness": 0.0,
      "semantic_accuracy": 0.0,
      "best_practices": 0.0,
      "plan_quality": 0.0,
      "execution_time_ms": 0.0,
      "rows_returned": 0,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": "no such column: order_date",
      "suggestions": []
    },
    {
      "task_id": "ent_complex_join_10_tables",
      "question": "Complex 10-table join for comprehensive order analysis",
      "difficulty": "enterprise",
      "tags": [
        "join",
        "complex",
        "star_schema",
        "multi_table"
      ],
      "gold_sql": "SELECT o.order_id, c.customer_name, c.segment as customer_segment, p.product_name, p.category as product_category, s.store_name, s.region as store_region, d.full_date as order_date, d.day_of_week, d.is_holiday, pr.promotion_name, pr.discount_pct, sh.carrier, sh.shipping_method, pay.payment_method, pay.payment_status, o.quantity, o.unit_price, o.quantity * o.unit_price as line_total, COALESCE(pr.discount_pct, 0) * o.quantity * o.unit_price / 100 as discount_amount FROM orders_fact o JOIN dim_customer c ON o.customer_id = c.customer_id JOIN dim_product p ON o.product_id = p.product_id JOIN dim_store s ON o.store_id = s.store_id JOIN dim_date d ON o.date_id = d.date_id LEFT JOIN dim_promotion pr ON o.promotion_id = pr.promotion_id LEFT JOIN shipping sh ON o.order_id = sh.order_id LEFT JOIN payments pay ON o.order_id = pay.order_id WHERE d.year = 2024 AND c.segment = 'Enterprise' ORDER BY o.order_id LIMIT 1000",
      "agent_sql": "SELECT o.order_id, c.customer_name, c.segment as customer_segment, p.product_name, p.category as product_category, s.store_name, s.region as store_region, d.full_date as order_date, d.day_of_week, d.is_holiday, pr.promotion_name, pr.discount_pct, sh.carrier, sh.shipping_method, pay.payment_method, pay.payment_status, o.quantity, o.unit_price, o.quantity * o.unit_price as line_total, COALESCE(pr.discount_pct, 0) * o.quantity * o.unit_price / 100 as discount_amount FROM orders_fact o JOIN dim_customer c ON o.customer_id = c.customer_id JOIN dim_product p ON o.product_id = p.product_id JOIN dim_store s ON o.store_id = s.store_id JOIN dim_date d ON o.date_id = d.date_id LEFT JOIN dim_promotion pr ON o.promotion_id = pr.promotion_id LEFT JOIN shipping sh ON o.order_id = sh.order_id LEFT JOIN payments pay ON o.order_id = pay.order_id WHERE d.year = 2024 AND c.segment = 'Enterprise' ORDER BY o.order_id LIMIT 1000",
      "status": "failed",
      "overall_score": 0.0,
      "correctness": 0.0,
      "efficiency": 0.0,
      "safety": 0.0,
      "completeness": 0.0,
      "semantic_accuracy": 0.0,
      "best_practices": 0.0,
      "plan_quality": 0.0,
      "execution_time_ms": 0.0,
      "rows_returned": 0,
      "is_valid": true,
      "validation_errors": [],
      "phantom_tables": [],
      "phantom_columns": [],
      "matches_expected": null,
      "match_score": 0.0,
      "error_message": "no such column: o.unit_price",
      "suggestions": []
    }
  ]
}